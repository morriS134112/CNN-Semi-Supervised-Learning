{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNw3MKu8hlEK",
        "outputId": "2f80adfd-3d14-4a8b-d622-688b697ddb58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1awF7pZ9Dz7X1jn1_QAiKN-_v56veCEKy\n",
            "From (redirected): https://drive.google.com/uc?id=1awF7pZ9Dz7X1jn1_QAiKN-_v56veCEKy&confirm=t&uuid=2627fc3f-7861-4ecc-a986-7754eed27213\n",
            "To: /content/food-11.zip\n",
            "100% 963M/963M [00:13<00:00, 71.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "# You may choose where to download the data.\n",
        "\n",
        "# Google Drive\n",
        "!gdown --id '1awF7pZ9Dz7X1jn1_QAiKN-_v56veCEKy' --output food-11.zip\n",
        "\n",
        "# Dropbox\n",
        "# !wget https://www.dropbox.com/s/m9q6273jl3djall/food-11.zip -O food-11.zip\n",
        "\n",
        "# MEGA\n",
        "#!sudo apt install megatools\n",
        "#!megadl \"https://mega.nz/#!zt1TTIhK!ZuMbg5ZjGWzWX1I6nEUbfjMZgCmAgeqJlwDkqdIryfg\"\n",
        "\n",
        "# Unzip the dataset.\n",
        "# This may take some time.\n",
        "!unzip -q food-11.zip\n",
        "# Import necessary packages.\n",
        "import numpy as np\n",
        "import torch,torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
        "from torchvision.datasets import DatasetFolder\n",
        "\n",
        "# This is for the progress bar.\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O9L7fPENhlEL"
      },
      "outputs": [],
      "source": [
        "# It is important to do data augmentation in training.\n",
        "# However, not every augmentation is useful.\n",
        "# Please think about what kind of augmentation is helpful for food recognition.\n",
        "train_tfm = transforms.Compose([\n",
        "    # Resize the image into a fixed shape (height = width = 128)\n",
        "    # You may add some transforms here.\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.RandomRotation(30),\n",
        "    # ToTensor() should be the last one of the transforms.\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# We don't need augmentations in testing and validation.\n",
        "# All we need here is to resize the PIL image and transform it into Tensor.\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YUJF4z2YhlEM"
      },
      "outputs": [],
      "source": [
        "# Batch size for training, validation, and testing.\n",
        "# A greater batch size usually gives a more stable gradient.\n",
        "# But the GPU memory is limited, so please adjust it carefully.\n",
        "batch_size = 16\n",
        "\n",
        "# Construct datasets.\n",
        "# The argument \"loader\" tells how torchvision reads the data.\n",
        "# 2117 2928 train\n",
        "train_set = DatasetFolder(\"food-11/training/labeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\n",
        "valid_set = DatasetFolder(\"food-11/validation\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\n",
        "unlabeled_set = DatasetFolder(\"food-11/training/unlabeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\n",
        "test_set = DatasetFolder(\"food-11/testing\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\n",
        "\n",
        "# Construct data loaders.\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WKQIzrAZhlEM"
      },
      "outputs": [],
      "source": [
        "class Classifier1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier1, self).__init__()\n",
        "        # The arguments for commonly used modules:\n",
        "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
        "\n",
        "        # input image size: [3, 128, 128]\n",
        "\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            # 3 * 224 * 224 -> 64 * 111 * 111\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            # 64 * 111 * 111 -> 128 * 54 * 54\n",
        "            nn.Conv2d(64, 128, 3),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            # 128 * 54 * 54 -> 256 * 26 * 26\n",
        "            nn.Conv2d(128, 256, 3),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            # 256 * 12 * 12  -> 512 * 5 * 5\n",
        "            nn.Conv2d(256, 512, 3),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(512 * 5 * 5, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, 11),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input (x): [batch_size, 3, 128, 128]\n",
        "        # output: [batch_size, 11]\n",
        "\n",
        "        # Extract features by convolutional layers.\n",
        "        x = self.cnn_layers(x)\n",
        "\n",
        "        # The extracted feature map must be flatten before going to fully-connected layers.\n",
        "        x = x.flatten(1)\n",
        "\n",
        "        # The features are transformed by fully-connected layers to obtain the final logits.\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k311Kh3vhlEN"
      },
      "outputs": [],
      "source": [
        "class Classifier2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier2, self).__init__()\n",
        "        # The arguments for commonly used modules:\n",
        "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
        "\n",
        "        # input image size: [3, 128, 128]\n",
        "\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            # 3 * 224 * 224 -> 64 * 111 * 111\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            # 64 * 111 * 111 -> 128 * 54 * 54\n",
        "            nn.Conv2d(64, 128, 3),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            # 128 * 54 * 54 -> 256 * 26 * 26\n",
        "            nn.Conv2d(128, 256, 3),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            # 256 * 26 * 26  -> 256 * 12 * 12\n",
        "            nn.Conv2d(256, 256, 3),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            # 256 * 12 * 12  -> 512 * 5 * 5\n",
        "            nn.Conv2d(256, 512, 3),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(512 * 5 * 5, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(512, 11),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input (x): [batch_size, 3, 128, 128]\n",
        "        # output: [batch_size, 11]\n",
        "\n",
        "        # Extract features by convolutional layers.\n",
        "        x = self.cnn_layers(x)\n",
        "\n",
        "        # The extracted feature map must be flatten before going to fully-connected layers.\n",
        "        x = x.flatten(1)\n",
        "\n",
        "        # The features are transformed by fully-connected layers to obtain the final logits.\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b9214f0fbdf940428ef2d1307bb2598b",
            "5a1e9164369d44eab00ce818d9447832",
            "28dd197b18e842d29ebdb3f264fa71f9",
            "e7fe4184d7a94539a313fbaaa94c3812",
            "705790659abe431f821f09129348dcba",
            "06952f7d6f5e442e9265968dfa375e7e",
            "a44222a2e0e245bf9ab199f66c82adca",
            "a0f518f7320a4175a58c018b74404b15",
            "7893b028d9f240d6a91e7dbff0905109",
            "75d5234d2fa4444284cc710ca1ed1115",
            "06c32f62c55649bfbe8bc6ebcc5a0b6f"
          ]
        },
        "id": "EqTOSnlehlEN",
        "outputId": "40fc4567-0d76-4cbe-f41c-0259a1dd2a13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/210 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9214f0fbdf940428ef2d1307bb2598b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Make sure the model is in eval mode.\n",
        "# Some modules like Dropout or BatchNorm affect if the model is in training mode.\n",
        "model1 = Classifier1().to(device)\n",
        "model1.load_state_dict(torch.load('./model1.ckpt'))\n",
        "model2 = Classifier2().to(device)\n",
        "model2.load_state_dict(torch.load('./model4.ckpt'))\n",
        "model1.eval()\n",
        "model2.eval()\n",
        "\n",
        "# Initialize a list to store the predictions.\n",
        "predictions = []\n",
        "\n",
        "# Iterate the testing set by batches.\n",
        "for batch in tqdm(test_loader):\n",
        "    # A batch consists of image data and corresponding labels.\n",
        "    # But here the variable \"labels\" is useless since we do not have the ground-truth.\n",
        "    # If printing out the labels, you will find that it is always 0.\n",
        "    # This is because the wrapper (DatasetFolder) returns images and labels for each batch,\n",
        "    # so we have to create fake labels to make it work normally.\n",
        "    imgs, labels = batch\n",
        "\n",
        "    # We don't need gradient in testing, and we don't even have labels to compute loss.\n",
        "    # Using torch.no_grad() accelerates the forward process.\n",
        "    with torch.no_grad():\n",
        "        logits1 = model1(imgs.to(device))\n",
        "        logits2 = model2(imgs.to(device))\n",
        "        logits = (logits1 + logits2) / 2\n",
        "\n",
        "    # Take the class with greatest logit as prediction and record it.\n",
        "    predictions.extend(logits.argmax(dim=-1).cpu().numpy().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t5jA6EA8hlEO"
      },
      "outputs": [],
      "source": [
        "# Save predictions into the file.\n",
        "with open(\"Ensemble2.csv\", \"w\") as f:\n",
        "\n",
        "    # The first row must be \"Id, Category\"\n",
        "    f.write(\"Id,Category\\n\")\n",
        "\n",
        "    # For the rest of the rows, each image id corresponds to a predicted class.\n",
        "    for i, pred in  enumerate(predictions):\n",
        "         f.write(f\"{i},{pred}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abLe1etBhlEO"
      },
      "source": [
        "(kaggle public 0.81899)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nzNsgUKhlEP"
      },
      "source": [
        "# References\n",
        "\n",
        "I use the sample code from TA for this course, and reference the official documentation https://pytorch.org/vision/stable/transforms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp60iuKjhlEP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b9214f0fbdf940428ef2d1307bb2598b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a1e9164369d44eab00ce818d9447832",
              "IPY_MODEL_28dd197b18e842d29ebdb3f264fa71f9",
              "IPY_MODEL_e7fe4184d7a94539a313fbaaa94c3812"
            ],
            "layout": "IPY_MODEL_705790659abe431f821f09129348dcba"
          }
        },
        "5a1e9164369d44eab00ce818d9447832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06952f7d6f5e442e9265968dfa375e7e",
            "placeholder": "​",
            "style": "IPY_MODEL_a44222a2e0e245bf9ab199f66c82adca",
            "value": "100%"
          }
        },
        "28dd197b18e842d29ebdb3f264fa71f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0f518f7320a4175a58c018b74404b15",
            "max": 210,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7893b028d9f240d6a91e7dbff0905109",
            "value": 210
          }
        },
        "e7fe4184d7a94539a313fbaaa94c3812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75d5234d2fa4444284cc710ca1ed1115",
            "placeholder": "​",
            "style": "IPY_MODEL_06c32f62c55649bfbe8bc6ebcc5a0b6f",
            "value": " 210/210 [00:34&lt;00:00,  7.45it/s]"
          }
        },
        "705790659abe431f821f09129348dcba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06952f7d6f5e442e9265968dfa375e7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a44222a2e0e245bf9ab199f66c82adca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0f518f7320a4175a58c018b74404b15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7893b028d9f240d6a91e7dbff0905109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75d5234d2fa4444284cc710ca1ed1115": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06c32f62c55649bfbe8bc6ebcc5a0b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}